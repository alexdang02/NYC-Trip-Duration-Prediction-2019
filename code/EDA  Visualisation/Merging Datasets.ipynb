{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Merging datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "root_folder = \"/home/trungdc/unimelb/MAST30024/asm/mast30034_2021_s2_project_1-alexdang02-1/\"\n",
    "data_dir = os.path.join(root_folder, \"Data\")\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir,\"Trip\", \"yellow_concat.csv\"))\n",
    "sdf= sdf.where((sdf.PULocationID.isin([\"264\", \"265\"]) == False) | (sdf.PULocationID.isin([\"264\", \"265\"]) == False) ) \\\n",
    ".withColumn(\"tpep_pickup_datetime\", to_timestamp(sdf.tpep_pickup_datetime, 'yyyy-MM-dd HH:mm:ss') ) \\\n",
    ".withColumn(\"tpep_dropoff_datetime\", to_timestamp(sdf.tpep_dropoff_datetime, 'yyyy-MM-dd HH:mm:ss') ) \\\n",
    ".withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"int\")) \\\n",
    ".withColumn(\"duration(m)\", round((col(\"tpep_dropoff_datetime\").cast(\"long\")  - col(\"tpep_pickup_datetime\").cast(\"long\"))/60)) \\\n",
    ".withColumn('Key', concat(col('PULocationID'),lit('-'), col('DOLocationID'))) \\\n",
    ".withColumn(\"date\", date_trunc(\"day\", col(\"tpep_pickup_datetime\"))) \\\n",
    ".drop(\"store_and_fwd_flag\", \"tpep_dropoff_datetime\")\n",
    "sdf.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+-------+-------------------+\n",
      "|VendorID|tpep_pickup_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|duration(m)|    Key|               date|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+-------+-------------------+\n",
      "|       1| 2018-01-01 00:44:55|              1|            2|         1|         239|         140|           2|         14|  0.5|    0.5|         0|           0|                  0.3|        15.3|       18.0|239-140|2018-01-01 00:00:00|\n",
      "|       1| 2018-01-01 00:20:22|              1|           10|         1|         140|         257|           2|       33.5|  0.5|    0.5|         0|           0|                  0.3|        34.8|       32.0|140-257|2018-01-01 00:00:00|\n",
      "|       1| 2018-01-01 00:56:38|              1|            1|         1|         238|          24|           1|        5.5|  0.5|    0.5|       1.7|           0|                  0.3|         8.5|        4.0| 238-24|2018-01-01 00:00:00|\n",
      "|       1| 2018-01-01 00:52:54|              1|            3|         1|         141|         113|           2|       16.5|  0.5|    0.5|         0|           0|                  0.3|        17.8|       25.0|141-113|2018-01-01 00:00:00|\n",
      "|       1| 2018-01-01 00:31:11|              1|           10|         1|         229|          61|           2|         35|  0.5|    0.5|         0|           0|                  0.3|        36.3|       37.0| 229-61|2018-01-01 00:00:00|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "taxi_zones =spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir, \"OSRM\", \"OSRM_plus.csv\") ) \\\n",
    "    .withColumnRenamed(\"expected_total_distance\", \"expected_total_distance(miles)\")\\\n",
    "    .withColumnRenamed(\"expected_total_duration\", \"expected_total_duration(s)\")\\\n",
    "    .withColumn(\"expected_total_distance(miles)\", round(\"expected_total_distance(miles)\", 1)) \\\n",
    "    .drop(\"geometry\", \"expected_main_road\", \"_c0\",\"Expected_AVG_speed\") \n",
    "taxi_zones.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+--------------------------+---+\n",
      "|expected_total_distance(miles)|expected_total_duration(s)|Key|\n",
      "+------------------------------+--------------------------+---+\n",
      "|                          28.8|                    2806.0|1-2|\n",
      "|                          38.0|                    3823.1|1-3|\n",
      "|                          20.9|                    2488.2|1-4|\n",
      "|                           7.8|                    1251.2|1-5|\n",
      "|                           8.2|                    1115.9|1-6|\n",
      "+------------------------------+--------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "weather = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir, \"Weather\", \"Weather processed.csv\") ) \\\n",
    ".withColumn(\"date\", to_timestamp(\"date\", 'yyyy-MM-dd') ) \n",
    "weather.show(5)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-------+-------+-------+-------------+----+---+-------------+-------+---------+-------------------+\n",
      "|_c0|tempMax|tempMin|tempAvg|tempDeparture| hdd|cdd|precipitation|newSnow|snowDepth|               date|\n",
      "+---+-------+-------+-------+-------------+----+---+-------------+-------+---------+-------------------+\n",
      "|  0|   19.0|    6.8|   12.9|        -21.9|51.8|0.0|          0.0|    0.0|      0.0|2018-01-01 00:00:00|\n",
      "|  1|   26.7|   12.8|   19.8|        -14.9|44.8|0.0|          0.0|    0.0|      0.0|2018-01-02 00:00:00|\n",
      "|  2|   29.3|   12.5|   20.9|        -13.5|44.0|0.0|          0.0|    0.0|      0.0|2018-01-03 00:00:00|\n",
      "|  3|   29.0|   19.0|   24.0|        -10.2|40.8|0.0|          0.6|    8.3|      1.0|2018-01-04 00:00:00|\n",
      "|  4|   19.0|    9.2|   14.1|        -20.0|50.8|0.0|          0.1|    0.0|      7.4|2018-01-05 00:00:00|\n",
      "+---+-------+-------+-------+-------------+----+---+-------------+-------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(f\"Number of row before joined {sdf.count()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of row before joined 55387165\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "trip_taxi = sdf.join(taxi_zones, sdf.Key == taxi_zones.Key, \"inner\") \n",
    "merge_dataset = trip_taxi.join(weather, trip_taxi.date == weather.date, \"inner\") \n",
    "merge_dataset.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------+-------------------+------------------------------+--------------------------+------+---+-------+-------+-------+-------------+----+----+-------------+-------+---------+-------------------+\n",
      "|VendorID|tpep_pickup_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|duration(m)|   Key|               date|expected_total_distance(miles)|expected_total_duration(s)|   Key|_c0|tempMax|tempMin|tempAvg|tempDeparture| hdd| cdd|precipitation|newSnow|snowDepth|               date|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------+-------------------+------------------------------+--------------------------+------+---+-------+-------+-------+-------------+----+----+-------------+-------+---------+-------------------+\n",
      "|       2| 2018-08-07 09:52:38|              2|           12|         3|           1|         156|           2|         52|    0|      0|         0|       18.25|                  0.3|       70.55|       18.0| 1-156|2018-08-07 00:00:00|                           3.0|                     714.7| 1-156|218|   89.7|   74.2|   81.9|          4.8| 0.0|17.2|          0.4|    0.0|      0.0|2018-08-07 00:00:00|\n",
      "|       1| 2018-01-04 05:20:05|              1|           16|         1|         100|          11|           2|       44.5|  0.5|    0.5|         0|           0|                  0.3|        45.8|       24.0|100-11|2018-01-04 00:00:00|                          15.0|                    1670.4|100-11|  3|   29.0|   19.0|   24.0|        -10.2|40.8| 0.0|          0.6|    8.3|      1.0|2018-01-04 00:00:00|\n",
      "|       1| 2018-01-05 20:07:25|              1|           13|         1|         100|          11|           1|       40.5|  0.5|    0.5|       9.5|        5.76|                  0.3|       57.06|       36.0|100-11|2018-01-05 00:00:00|                          15.0|                    1670.4|100-11|  4|   19.0|    9.2|   14.1|        -20.0|50.8| 0.0|          0.1|    0.0|      7.4|2018-01-05 00:00:00|\n",
      "|       1| 2018-01-06 04:14:51|              1|           17|         1|         100|          11|           1|         49|  0.5|    0.5|         1|           0|                  0.3|        51.3|       31.0|100-11|2018-01-06 00:00:00|                          15.0|                    1670.4|100-11|  5|   13.8|    6.7|   10.2|        -23.7|54.3| 0.0|          0.0|    0.0|      7.2|2018-01-06 00:00:00|\n",
      "|       1| 2018-02-10 00:46:31|              1|           17|         1|         100|          11|           1|         50|  0.5|    0.5|         5|           0|                  0.3|        56.3|       39.0|100-11|2018-02-10 00:00:00|                          15.0|                    1670.4|100-11| 40|   48.8|   35.7|   42.2|          8.0|22.5| 0.0|          0.5|    0.0|      0.0|2018-02-10 00:00:00|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------+-------------------+------------------------------+--------------------------+------+---+-------+-------+-------+-------------+----+----+-------------+-------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "merge_dataset = merge_dataset.drop(\"date\", \"Key\", \"_c0\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(f\"Number of row after joined {merge_dataset.count()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of row after joined 55142680\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "for col in merge_dataset.dtypes:\n",
    "    print(f\"Column {col[0]} is of type {col[1]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Column VendorID is of type string\n",
      "Column tpep_pickup_datetime is of type timestamp\n",
      "Column passenger_count is of type string\n",
      "Column trip_distance is of type int\n",
      "Column RatecodeID is of type string\n",
      "Column PULocationID is of type string\n",
      "Column DOLocationID is of type string\n",
      "Column payment_type is of type string\n",
      "Column fare_amount is of type string\n",
      "Column extra is of type string\n",
      "Column mta_tax is of type string\n",
      "Column tip_amount is of type string\n",
      "Column tolls_amount is of type string\n",
      "Column improvement_surcharge is of type string\n",
      "Column total_amount is of type string\n",
      "Column duration(m) is of type double\n",
      "Column Key is of type string\n",
      "Column date is of type timestamp\n",
      "Column expected_total_distance(miles) is of type double\n",
      "Column expected_total_duration(s) is of type string\n",
      "Column Key is of type string\n",
      "Column _c0 is of type string\n",
      "Column tempMax is of type string\n",
      "Column tempMin is of type string\n",
      "Column tempAvg is of type string\n",
      "Column tempDeparture is of type string\n",
      "Column hdd is of type string\n",
      "Column cdd is of type string\n",
      "Column precipitation is of type string\n",
      "Column newSnow is of type string\n",
      "Column snowDepth is of type string\n",
      "Column date is of type timestamp\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train and test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## USe 2018 data as trainset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "data2018 = merge_dataset.filter(merge_dataset.tpep_pickup_datetime > \"2018-01-01\").filter(merge_dataset.tpep_pickup_datetime < \"2019-01-01\")\n",
    "data2018.show(5)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------------------------+--------------------------+-------+-------+-------+-------------+----+----+-------------+-------+---------+\n",
      "|VendorID|tpep_pickup_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|duration(m)|expected_total_distance(miles)|expected_total_duration(s)|tempMax|tempMin|tempAvg|tempDeparture| hdd| cdd|precipitation|newSnow|snowDepth|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------------------------+--------------------------+-------+-------+-------+-------------+----+----+-------------+-------+---------+\n",
      "|       2| 2018-08-07 09:52:38|              2|           12|         3|           1|         156|           2|         52|    0|      0|         0|       18.25|                  0.3|       70.55|       18.0|                           3.0|                     714.7|   89.7|   74.2|   81.9|          4.8| 0.0|17.2|          0.4|    0.0|      0.0|\n",
      "|       1| 2018-01-04 05:20:05|              1|           16|         1|         100|          11|           2|       44.5|  0.5|    0.5|         0|           0|                  0.3|        45.8|       24.0|                          15.0|                    1670.4|   29.0|   19.0|   24.0|        -10.2|40.8| 0.0|          0.6|    8.3|      1.0|\n",
      "|       1| 2018-01-05 20:07:25|              1|           13|         1|         100|          11|           1|       40.5|  0.5|    0.5|       9.5|        5.76|                  0.3|       57.06|       36.0|                          15.0|                    1670.4|   19.0|    9.2|   14.1|        -20.0|50.8| 0.0|          0.1|    0.0|      7.4|\n",
      "|       1| 2018-01-06 04:14:51|              1|           17|         1|         100|          11|           1|         49|  0.5|    0.5|         1|           0|                  0.3|        51.3|       31.0|                          15.0|                    1670.4|   13.8|    6.7|   10.2|        -23.7|54.3| 0.0|          0.0|    0.0|      7.2|\n",
      "|       1| 2018-02-10 00:46:31|              1|           17|         1|         100|          11|           1|         50|  0.5|    0.5|         5|           0|                  0.3|        56.3|       39.0|                          15.0|                    1670.4|   48.8|   35.7|   42.2|          8.0|22.5| 0.0|          0.5|    0.0|      0.0|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------------------------+--------------------------+-------+-------+-------+-------------+----+----+-------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "print(f\"Size of training set: {(data2018.count(), len(data2018.columns))}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of training set: (30206863, 27)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "data2018.repartition(1).write.csv(os.path.join(root_folder, \"Data\", \"Merge\", \"train.csv\"), header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use 2019 data as testset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "data2019 = merge_dataset.filter(merge_dataset.tpep_pickup_datetime > \"2018-12-31\").filter(merge_dataset.tpep_pickup_datetime < \"2020-01-01\")\n",
    "data2019.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------------------------+--------------------------+-------+-------+-------+-------------+----+---+-------------+-------+---------+\n",
      "|VendorID|tpep_pickup_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|duration(m)|expected_total_distance(miles)|expected_total_duration(s)|tempMax|tempMin|tempAvg|tempDeparture| hdd|cdd|precipitation|newSnow|snowDepth|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------------------------+--------------------------+-------+-------+-------+-------------+----+---+-------------+-------+---------+\n",
      "|       2| 2019-01-10 21:06:14|              1|           15|         1|         100|          11|           2|       49.5|  0.5|    0.5|         0|           0|                  0.3|        50.8|       50.0|                          15.0|                    1670.4|   35.2|   28.5|   31.8|         -1.6|33.0|0.0|          0.0|    0.0|      0.0|\n",
      "|       2| 2019-02-06 12:07:31|              1|           17|         1|         100|          11|           1|       56.5|    0|    0.5|         0|        5.76|                  0.3|       63.06|       57.0|                          15.0|                    1670.4|   46.5|   34.0|   40.2|          6.6|24.5|0.0|          0.5|    0.0|      0.0|\n",
      "|       1| 2019-03-06 00:58:22|              3|           12|         1|         100|          11|           2|         36|    3|    0.5|         0|           0|                  0.3|        39.8|       29.0|                          15.0|                    1670.4|   27.3|   18.8|   23.1|        -16.3|41.5|0.0|          0.0|    0.0|      1.0|\n",
      "|       1| 2019-04-04 23:41:25|              1|           15|         1|         100|          11|           1|       51.5|    3|    0.5|         5|        6.12|                  0.3|       66.42|       51.0|                          15.0|                    1670.4|   60.3|   42.2|   51.2|          3.0|13.3|0.0|          0.0|    0.0|      0.0|\n",
      "|       2| 2019-04-06 19:47:25|              1|           14|         1|         100|          11|           2|         46|    0|    0.5|         0|           0|                  0.3|        49.3|       48.0|                          15.0|                    1670.4|   66.0|   39.2|   52.6|          3.5|12.0|0.0|          0.0|    0.0|      0.0|\n",
      "+--------+--------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----------+------------------------------+--------------------------+-------+-------+-------+-------------+----+---+-------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "print(f\"Size of test set: {(data2019.count(), len(data2019.columns))}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of test set: (24997342, 32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "data2019.repartition(1).write.csv(os.path.join(root_folder, \"Data\", \"Merge\", \"test.csv\"), header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean up folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for file in os.listdir(os.path.join(root_folder, \"Data\", \"Merge\", \"test.csv\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(root_folder, \"Data\", \"Merge\", \"test.csv\", file),os.path.join( root_folder, \"Data\", \"Merge\", \"test.csv\"))\n",
    "        shutil.rmtree(os.path.join(root_folder, \"Data\", \"Merge\", \"test.csv\"))\n",
    "for file in os.listdir(os.path.join(root_folder, \"Data\", \"Merge\", \"train.csv\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(root_folder, \"Data\", \"Merge\", \"train.csv\", file),os.path.join( root_folder, \"Data\", \"Merge\", \"train.csv\"))\n",
    "        shutil.rmtree(os.path.join(root_folder, \"Data\", \"Merge\", \"train.csv\"))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mast30024asm1': conda)"
  },
  "interpreter": {
   "hash": "3ee96da6bb92bbf5b9d339a3dfb1419b22df50abca65b46f31ce3f2043457e99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}