{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pyspark\n",
    "import os\n",
    "import shutil\n",
    "root_folder = \"/home/trungdc/unimelb/MAST30024/asm/project 1/\"\n",
    "data_dir = os.path.join(root_folder, \"Data/Trip\")\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download yellow trip record to folder: Data/yellow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for file in sorted(os.listdir(os.path.join(data_dir, \"yellow\"))):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "    sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir,\"yellow\",file))\n",
    "    size_before = sdf.count()\n",
    "    while True:\n",
    "        sample = sdf.sample(withReplacement=False, fraction=0.3)\n",
    "        size_after = sample.count()\n",
    "        if 0.2 < size_after/size_before <= 0.3:\n",
    "            print(f\"File {file} is sampled with size {sample.count()/size_before} or {size_after} rows \")\n",
    "            sample.repartition(1).write.csv(os.path.join(data_dir,\"sample\", file), header=True)\n",
    "            for outfile in os.listdir(os.path.join(data_dir, \"sample\", file)):\n",
    "                if outfile.endswith(\".csv\"):\n",
    "                    os.rename(os.path.join(data_dir, \"sample\", file,outfile), os.path.join(data_dir, \"after_sample\", file ))\n",
    "            shutil.rmtree(os.path.join(data_dir, \"sample\" ,file ))\n",
    "            os.remove(os.path.join(data_dir, \"yellow\" ,file ))\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File yellow_tripdata_2018-01.csv is sampled with size 0.2999213230692588 or 2627273 rows \n",
      "File yellow_tripdata_2018-02.csv is sampled with size 0.2997702799645222 or 2545672 rows \n",
      "File yellow_tripdata_2018-03.csv is sampled with size 0.2999833728792998 or 2828956 rows \n",
      "File yellow_tripdata_2018-04.csv is sampled with size 0.29981102604208365 or 2789896 rows \n",
      "File yellow_tripdata_2018-05.csv is sampled with size 0.2998641704853924 or 2765966 rows \n",
      "File yellow_tripdata_2018-06.csv is sampled with size 0.29986546675050274 or 2612977 rows \n",
      "File yellow_tripdata_2018-07.csv is sampled with size 0.29974006808881 or 2352884 rows \n",
      "File yellow_tripdata_2018-08.csv is sampled with size 0.2999840491957457 or 2354615 rows \n",
      "File yellow_tripdata_2018-09.csv is sampled with size 0.2998742682490451 or 2411029 rows \n",
      "File yellow_tripdata_2018-10.csv is sampled with size 0.2999733026644621 or 2646096 rows \n",
      "File yellow_tripdata_2018-11.csv is sampled with size 0.2997749339362596 or 2441716 rows \n",
      "File yellow_tripdata_2018-12.csv is sampled with size 0.299897433462972 or 2451131 rows \n",
      "File yellow_tripdata_2019-01.csv is sampled with size 0.29993771349040244 or 2299860 rows \n",
      "File yellow_tripdata_2019-02.csv is sampled with size 0.29995989671445106 or 2105531 rows \n",
      "File yellow_tripdata_2019-03.csv is sampled with size 0.29980868287382967 or 2348265 rows \n",
      "File yellow_tripdata_2019-04.csv is sampled with size 0.29980806224664974 or 2228515 rows \n",
      "File yellow_tripdata_2019-05.csv is sampled with size 0.29968972650117426 or 2267231 rows \n",
      "File yellow_tripdata_2019-06.csv is sampled with size 0.2997272448560904 or 2080414 rows \n",
      "File yellow_tripdata_2019-07.csv is sampled with size 0.2999786543492595 or 1892991 rows \n",
      "File yellow_tripdata_2019-08.csv is sampled with size 0.29982824984600775 or 1820964 rows \n",
      "File yellow_tripdata_2019-09.csv is sampled with size 0.29987128086351145 or 1969491 rows \n",
      "File yellow_tripdata_2019-10.csv is sampled with size 0.2999650258092339 or 2163915 rows \n",
      "File yellow_tripdata_2019-11.csv is sampled with size 0.29992813433804716 or 2062939 rows \n",
      "File yellow_tripdata_2019-12.csv is sampled with size 0.2997837831410592 or 2067404 rows \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concaternate sampled data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir,\"after_sample\",\"*.csv\"))\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "sdf.createOrReplaceTempView(\"trip\")\n",
    "sdf.limit(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
       "|       1| 2018-01-01 00:44:55|  2018-01-01 01:03:05|              1|         2.70|         1|                 N|         239|         140|           2|         14|  0.5|    0.5|         0|           0|                  0.3|        15.3|\n",
       "|       1| 2018-01-01 00:20:22|  2018-01-01 00:52:51|              1|        10.20|         1|                 N|         140|         257|           2|       33.5|  0.5|    0.5|         0|           0|                  0.3|        34.8|\n",
       "|       1| 2018-01-01 00:56:38|  2018-01-01 01:01:05|              1|         1.00|         1|                 N|         238|          24|           1|        5.5|  0.5|    0.5|       1.7|           0|                  0.3|         8.5|\n",
       "|       1| 2018-01-01 00:52:54|  2018-01-01 01:17:33|              1|         3.50|         1|                 N|         141|         113|           2|       16.5|  0.5|    0.5|         0|           0|                  0.3|        17.8|\n",
       "|       1| 2018-01-01 00:31:11|  2018-01-01 01:07:56|              1|        10.90|         1|                 N|         229|          61|           2|         35|  0.5|    0.5|         0|           0|                  0.3|        36.3|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:44:55</td><td>2018-01-01 01:03:05</td><td>1</td><td>2.70</td><td>1</td><td>N</td><td>239</td><td>140</td><td>2</td><td>14</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>15.3</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:20:22</td><td>2018-01-01 00:52:51</td><td>1</td><td>10.20</td><td>1</td><td>N</td><td>140</td><td>257</td><td>2</td><td>33.5</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>34.8</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:56:38</td><td>2018-01-01 01:01:05</td><td>1</td><td>1.00</td><td>1</td><td>N</td><td>238</td><td>24</td><td>1</td><td>5.5</td><td>0.5</td><td>0.5</td><td>1.7</td><td>0</td><td>0.3</td><td>8.5</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:52:54</td><td>2018-01-01 01:17:33</td><td>1</td><td>3.50</td><td>1</td><td>N</td><td>141</td><td>113</td><td>2</td><td>16.5</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>17.8</td></tr>\n",
       "<tr><td>1</td><td>2018-01-01 00:31:11</td><td>2018-01-01 01:07:56</td><td>1</td><td>10.90</td><td>1</td><td>N</td><td>229</td><td>61</td><td>2</td><td>35</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>36.3</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(f\"Size of data after being sample: {sdf.count(), len(sdf.columns)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of data after being sample: (56135731, 17)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM trip\n",
    "ORDER BY \"tpep_pickup_datetime\"\n",
    "\"\"\"\n",
    "file = \"yellow_concat.csv\"\n",
    "out = spark.sql(sql_query)\n",
    "out.head()\n",
    "out.repartition(1).write.csv(os.path.join(data_dir, file), header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Note\n",
    "- Go to this directory: root_folder/Trip/Data/yellow_concat.csv/\n",
    "- Inside there will be a single csv file with strange name\n",
    "- Move the file to this directory root_folder/Trip/Data/\n",
    "- Delete the folder: root_folder/Trip/Data/yellow_concat.csv/\n",
    "- Rename that strange-name csv file to yellow_concat.csv\n",
    "\n",
    "Sorry for this inconvenience. I try to automate but run into a bug that I can not fix"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mast30024asm1': conda)"
  },
  "interpreter": {
   "hash": "3ee96da6bb92bbf5b9d339a3dfb1419b22df50abca65b46f31ce3f2043457e99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}