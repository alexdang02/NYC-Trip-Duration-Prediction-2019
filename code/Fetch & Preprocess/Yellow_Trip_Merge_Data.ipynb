{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pyspark\n",
    "import os\n",
    "import shutil\n",
    "root_folder = \"/home/trungdc/unimelb/MAST30024/asm/mast30034_2021_s2_project_1-alexdang02-1/\"\n",
    "data_dir = os.path.join(root_folder, \"Data/Trip\")\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download yellow trip record to folder: Data/yellow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for file in sorted(os.listdir(os.path.join(data_dir, \"yellow\"))):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "    sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir,\"yellow\",file))\n",
    "    size_before = sdf.count()\n",
    "    while True:\n",
    "        sample = sdf.sample(withReplacement=False, fraction=0.3)\n",
    "        size_after = sample.count()\n",
    "        if 0.2 < size_after/size_before <= 0.3:\n",
    "            print(f\"File {file} is sampled with size {sample.count()/size_before} or {size_after} rows \")\n",
    "            sample.repartition(1).write.csv(os.path.join(data_dir,\"sample\", file), header=True)\n",
    "            for outfile in os.listdir(os.path.join(data_dir, \"sample\", file)):\n",
    "                if outfile.endswith(\".csv\"):\n",
    "                    os.rename(os.path.join(data_dir, \"sample\", file,outfile), os.path.join(data_dir, \"after_sample\", file ))\n",
    "            shutil.rmtree(os.path.join(data_dir, \"sample\" ,file ))\n",
    "            os.remove(os.path.join(data_dir, \"yellow\" ,file ))\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File yellow_tripdata_2018-01.csv is sampled with size 0.2999213230692588 or 2627273 rows \n",
      "File yellow_tripdata_2018-02.csv is sampled with size 0.2997702799645222 or 2545672 rows \n",
      "File yellow_tripdata_2018-03.csv is sampled with size 0.2999833728792998 or 2828956 rows \n",
      "File yellow_tripdata_2018-04.csv is sampled with size 0.29981102604208365 or 2789896 rows \n",
      "File yellow_tripdata_2018-05.csv is sampled with size 0.2998641704853924 or 2765966 rows \n",
      "File yellow_tripdata_2018-06.csv is sampled with size 0.29986546675050274 or 2612977 rows \n",
      "File yellow_tripdata_2018-07.csv is sampled with size 0.29974006808881 or 2352884 rows \n",
      "File yellow_tripdata_2018-08.csv is sampled with size 0.2999840491957457 or 2354615 rows \n",
      "File yellow_tripdata_2018-09.csv is sampled with size 0.2998742682490451 or 2411029 rows \n",
      "File yellow_tripdata_2018-10.csv is sampled with size 0.2999733026644621 or 2646096 rows \n",
      "File yellow_tripdata_2018-11.csv is sampled with size 0.2997749339362596 or 2441716 rows \n",
      "File yellow_tripdata_2018-12.csv is sampled with size 0.299897433462972 or 2451131 rows \n",
      "File yellow_tripdata_2019-01.csv is sampled with size 0.29993771349040244 or 2299860 rows \n",
      "File yellow_tripdata_2019-02.csv is sampled with size 0.29995989671445106 or 2105531 rows \n",
      "File yellow_tripdata_2019-03.csv is sampled with size 0.29980868287382967 or 2348265 rows \n",
      "File yellow_tripdata_2019-04.csv is sampled with size 0.29980806224664974 or 2228515 rows \n",
      "File yellow_tripdata_2019-05.csv is sampled with size 0.29968972650117426 or 2267231 rows \n",
      "File yellow_tripdata_2019-06.csv is sampled with size 0.2997272448560904 or 2080414 rows \n",
      "File yellow_tripdata_2019-07.csv is sampled with size 0.2999786543492595 or 1892991 rows \n",
      "File yellow_tripdata_2019-08.csv is sampled with size 0.29982824984600775 or 1820964 rows \n",
      "File yellow_tripdata_2019-09.csv is sampled with size 0.29987128086351145 or 1969491 rows \n",
      "File yellow_tripdata_2019-10.csv is sampled with size 0.2999650258092339 or 2163915 rows \n",
      "File yellow_tripdata_2019-11.csv is sampled with size 0.29992813433804716 or 2062939 rows \n",
      "File yellow_tripdata_2019-12.csv is sampled with size 0.2997837831410592 or 2067404 rows \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for file in sorted(os.listdir(os.path.join(data_dir, \"yellow\"))):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "    sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir,\"yellow\",file))\n",
    "    size_before = sdf.count()\n",
    "    while True:\n",
    "        sample = sdf.sample(withReplacement=False, fraction=0.3)\n",
    "        size_after = sample.count()\n",
    "        if 0.2 < size_after/size_before <= 0.3:\n",
    "            print(f\"File {file} is sampled with size {sample.count()/size_before} or {size_after} rows \")\n",
    "            sample.repartition(1).write.csv(os.path.join(data_dir,\"sample\", file), header=True)\n",
    "            for outfile in os.listdir(os.path.join(data_dir, \"sample\", file)):\n",
    "                if outfile.endswith(\".csv\"):\n",
    "                    os.rename(os.path.join(data_dir, \"sample\", file,outfile), os.path.join(data_dir, \"yellow_after\", file ))\n",
    "            shutil.rmtree(os.path.join(data_dir, \"sample\" ,file ))\n",
    "            os.remove(os.path.join(data_dir, \"yellow\" ,file ))\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File yellow_tripdata_2019-12.csv is sampled with size 0.2998718011367517 or 2068011 rows \n",
      "File yellow_tripdata_2020-01.csv is sampled with size 0.2998892775852134 or 5940235 rows \n",
      "File yellow_tripdata_2020-02.csv is sampled with size 0.2999190778335432 or 5803642 rows \n",
      "File yellow_tripdata_2020-03.csv is sampled with size 0.2999526637155667 or 6594534 rows \n",
      "File yellow_tripdata_2020-04.csv is sampled with size 0.2999571436247258 or 6313911 rows \n",
      "File yellow_tripdata_2020-05.csv is sampled with size 0.2998834908705921 or 6467195 rows \n",
      "File yellow_tripdata_2020-06.csv is sampled with size 0.2998749531766383 or 6337942 rows \n",
      "File yellow_tripdata_2020-07.csv is sampled with size 0.2998846615620915 or 6476442 rows \n",
      "File yellow_tripdata_2020-08.csv is sampled with size 0.2999285462081042 or 6633337 rows \n",
      "File yellow_tripdata_2020-09.csv is sampled with size 0.29982457099632503 or 6640341 rows \n",
      "File yellow_tripdata_2020-10.csv is sampled with size 0.2998564207740911 or 6982695 rows \n",
      "File yellow_tripdata_2020-11.csv is sampled with size 0.2998885812336028 or 6858594 rows \n",
      "File yellow_tripdata_2020-12.csv is sampled with size 0.2999327915518578 or 7154640 rows \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concaternate sampled data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(os.path.join(data_dir,\"yellow_after\",\"*.csv\"))\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "sdf.createOrReplaceTempView(\"trip\")\n",
    "sdf.limit(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
       "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
       "|       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|              1|         4.20|         1|                 N|         142|         116|           2|       14.5|    3|    0.5|         0|           0|                  0.3|        18.3|                 2.5|\n",
       "|       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|              1|          .00|         1|                 N|         145|         145|           2|        2.5|  0.5|    0.5|         0|           0|                  0.3|         3.8|                   0|\n",
       "|       1| 2019-12-01 00:19:48|  2019-12-01 00:24:18|              1|          .90|         1|                 N|         148|           4|           1|        5.5|    3|    0.5|      1.85|           0|                  0.3|       11.15|                 2.5|\n",
       "|       2| 2019-12-01 00:43:02|  2019-12-01 01:11:18|              1|        13.07|         1|                 N|          41|          51|           2|       38.5|  0.5|    0.5|         0|           0|                  0.3|        39.8|                   0|\n",
       "|       2| 2019-12-01 00:37:17|  2019-12-01 01:07:39|              5|        19.98|         2|                 N|         132|         238|           1|         52|    0|    0.5|     14.73|        6.12|                  0.3|       73.65|                   0|\n",
       "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+"
      ],
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th></tr>\n",
       "<tr><td>1</td><td>2019-12-01 00:26:58</td><td>2019-12-01 00:41:45</td><td>1</td><td>4.20</td><td>1</td><td>N</td><td>142</td><td>116</td><td>2</td><td>14.5</td><td>3</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>18.3</td><td>2.5</td></tr>\n",
       "<tr><td>1</td><td>2019-12-01 00:25:53</td><td>2019-12-01 00:26:04</td><td>1</td><td>.00</td><td>1</td><td>N</td><td>145</td><td>145</td><td>2</td><td>2.5</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>3.8</td><td>0</td></tr>\n",
       "<tr><td>1</td><td>2019-12-01 00:19:48</td><td>2019-12-01 00:24:18</td><td>1</td><td>.90</td><td>1</td><td>N</td><td>148</td><td>4</td><td>1</td><td>5.5</td><td>3</td><td>0.5</td><td>1.85</td><td>0</td><td>0.3</td><td>11.15</td><td>2.5</td></tr>\n",
       "<tr><td>2</td><td>2019-12-01 00:43:02</td><td>2019-12-01 01:11:18</td><td>1</td><td>13.07</td><td>1</td><td>N</td><td>41</td><td>51</td><td>2</td><td>38.5</td><td>0.5</td><td>0.5</td><td>0</td><td>0</td><td>0.3</td><td>39.8</td><td>0</td></tr>\n",
       "<tr><td>2</td><td>2019-12-01 00:37:17</td><td>2019-12-01 01:07:39</td><td>5</td><td>19.98</td><td>2</td><td>N</td><td>132</td><td>238</td><td>1</td><td>52</td><td>0</td><td>0.5</td><td>14.73</td><td>6.12</td><td>0.3</td><td>73.65</td><td>0</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(f\"Size of data after being sample: {sdf.count(), len(sdf.columns)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of data after being sample: (80271519, 18)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM trip\n",
    "ORDER BY \"tpep_pickup_datetime\"\n",
    "\"\"\"\n",
    "file = \"yellow2020.csv\"\n",
    "out = spark.sql(sql_query)\n",
    "out.head()\n",
    "out.repartition(1).write.csv(os.path.join(data_dir, file), header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Note\n",
    "- Go to this directory: root_folder/Trip/Data/yellow_concat.csv/\n",
    "- Inside there will be a single csv file with strange name\n",
    "- Move the file to this directory root_folder/Trip/Data/\n",
    "- Delete the folder: root_folder/Trip/Data/yellow_concat.csv/\n",
    "- Rename that strange-name csv file to yellow_concat.csv\n",
    "\n",
    "Sorry for this inconvenience. I try to automate but run into a bug that I can not fix"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mast30024asm1': conda)"
  },
  "interpreter": {
   "hash": "3ee96da6bb92bbf5b9d339a3dfb1419b22df50abca65b46f31ce3f2043457e99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}