{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import os\n",
    "import folium\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "root_folder = \"/home/trungdc/unimelb/MAST30024/asm/mast30034_2021_s2_project_1-alexdang02-1\"\n",
    "data_dir = os.path.join(root_folder, \"Data\")\n",
    "SQLOutput_dir = os.path.join(root_folder, \"code/SparkSQL_Output\")\n",
    "plot_dir = os.path.join(root_folder, \"Plots\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(os.path.join(data_dir,\"Merge\", \"train.csv\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def weekend(dayofweek):\n",
    "  if   dayofweek <= 5: \n",
    "      return 0\n",
    "  else:\n",
    "      return 1\n",
    "udfWeekendFunc = F.udf(weekend, IntegerType())\n",
    "\n",
    "def workingHour(hour, weekend):\n",
    "    if weekend <= 5:\n",
    "        if 7 <= hour <= 19:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "udfworkingHour = F.udf(workingHour, IntegerType())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def preprocess(sdf):\n",
    "    sdf = sdf.withColumn(\"tpep_pickup_datetime\", to_timestamp(sdf.tpep_pickup_datetime, 'yyyy-MM-dd HH:mm:ss') ) \\\n",
    "    .withColumnRenamed(\"duration(m)\", \"duration\")  \\\n",
    "    .withColumnRenamed('expected_total_distance(miles)', \"expected_total_distance\") \\\n",
    "    .withColumnRenamed('expected_total_duration(s)', \"expected_total_duration\") \n",
    "    sdf = sdf.withColumn(\"expected_total_duration\", sdf.expected_total_duration/60)\n",
    "    sdf = sdf.filter(sdf.passenger_count <= 6) \\\n",
    "        .filter(sdf.duration > 0) \\\n",
    "        .filter(sdf.PULocationID != sdf.DOLocationID) \\\n",
    "        .filter( ~ ((sdf.RatecodeID == 2) & (sdf.fare_amount <= 50))) \\\n",
    "        .filter(sdf.duration < 500) \\\n",
    "        .filter(sdf.total_amount >= 2.5) \\\n",
    "        .filter(sdf.tolls_amount >= 0) \\\n",
    "        .filter(sdf.VendorID.isin([1,2])) \\\n",
    "        .filter(sdf.RatecodeID.isin([1,2])) \\\n",
    "        .filter(sdf.payment_type.isin([1,2])) \\\n",
    "        .filter(sdf.tip_amount <= 25) \\\n",
    "    .withColumn(\"DayofWeek\", dayofweek(sdf.tpep_pickup_datetime))   \\\n",
    "    .withColumn(\"Weekend\", udfWeekendFunc(col(\"DayofWeek\"))) \\\n",
    "    .withColumn(\"Month\", month(sdf.tpep_pickup_datetime))   \\\n",
    "    .withColumn(\"Hour\", hour(sdf.tpep_pickup_datetime)) \n",
    "\n",
    "    sdf = sdf.withColumn(\"WorkingHour\", udfworkingHour(col(\"Hour\"), col(\"DayofWeek\"))) \\\n",
    "    .withColumn(\"trip_distance\",F.when(sdf.trip_distance==0, sdf.expected_total_distance).otherwise(sdf.trip_distance)) \\\n",
    "    .withColumn(\"passenger_count\", F.when(sdf.passenger_count==0, 1).when(sdf.passenger_count==6, 5).otherwise(sdf.passenger_count)) \\\n",
    "    .withColumn(\"tip_amount\", F.when(sdf.tip_amount<0, 0).otherwise(sdf.tip_amount)) \\\n",
    "    .withColumn(\"fare_amount\", F.when(sdf.fare_amount<2.5, 2.5).otherwise(sdf.fare_amount)) \\\n",
    "\n",
    "    .drop(\"expected_total_distance\", \"expected_total_duration\", \"tpep_pickup_datetime\", \"VendorID\")\n",
    "    return sdf\n",
    "sdf = preprocess(sdf)\n",
    "\n",
    "    # .withColumn(\"Key\", concat(sdf.DayofWeek,lit(\",\"), sdf.Hour )) \\"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "sdf.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------+-------+-------+-------+-------------+----+---+-------------+-------+---------+---------+-------+-----+----+-----------+----+\n",
      "|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|duration|tempMax|tempMin|tempAvg|tempDeparture| hdd|cdd|precipitation|newSnow|snowDepth|DayofWeek|Weekend|Month|Hour|WorkingHour| Key|\n",
      "+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------+-------+-------+-------+-------------+----+---+-------------+-------+---------+---------+-------+-----+----+-----------+----+\n",
      "|              3|         17.0|         1|         100|          11|           1|       50.5|  0.5|    0.5|       4.2|         0.0|                  0.3|        56.0|    41.0|   55.5|   42.7|   49.1|          6.8|15.5|0.0|          0.8|    0.0|      0.0|        1|      0|   12|   1|          0| 1,1|\n",
      "|              1|         16.0|         1|         100|          11|           2|       44.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        45.8|    24.0|   29.0|   19.0|   24.0|        -10.2|40.8|0.0|          0.6|    8.3|      1.0|        5|      0|    1|   5|          0| 5,5|\n",
      "|              2|         13.0|         1|         100|          11|           1|       43.5|  1.0|    0.5|     10.21|        5.76|                  0.3|       61.27|    45.0|   39.7|   26.7|   33.2|         -7.7|31.7|0.0|          0.0|    0.0|      0.0|        6|      1|   12|  16|          0|6,16|\n",
      "|              1|         13.0|         1|         100|          11|           1|       40.5|  0.5|    0.5|       9.5|        5.76|                  0.3|       57.06|    36.0|   19.0|    9.2|   14.1|        -20.0|50.8|0.0|          0.1|    0.0|      7.4|        6|      1|    1|  20|          0|6,20|\n",
      "|              1|         15.0|         1|         100|          11|           2|       49.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        50.8|    53.0|   46.8|   36.3|   41.6|          3.1|23.2|0.0|          1.3|    0.0|      0.0|        1|      0|   12|   0|          0| 1,0|\n",
      "+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------+-------+-------+-------+-------------+----+---+-------------+-------+---------+---------+-------+-----+----+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "sdf.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28064431"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "file = \"train1.csv\"\n",
    "size_before = sdf.count()\n",
    "print(size_before)\n",
    "sample = sdf.sample(withReplacement=False, fraction=0.1)\n",
    "size_after = sample.count()\n",
    "print(f\"File {file} is sampled with size {sample.count()/size_before} or {size_after} rows \")\n",
    "sample.repartition(1).write.csv(os.path.join(data_dir,\"Model\", file), header=True)\n",
    "for outfile in os.listdir(os.path.join(data_dir, \"Model\", file)):\n",
    "    if outfile.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(data_dir, \"Model\",file ), os.path.join(data_dir, \"Model\", file ))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "28064431\n",
      "File train1.csv is sampled with size 0.1000888990052925 or 2808938 rows \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create sampling test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sdf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(os.path.join(data_dir,\"Merge\", \"test.csv\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "sdf=  preprocess(sdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "file = \"test1.csv\"\n",
    "size_before = sdf.count()\n",
    "print(size_before)\n",
    "sample = sdf.sample(withReplacement=False, fraction=0.03)\n",
    "size_after = sample.count()\n",
    "print(f\"File {file} is sampled with size {sample.count()/size_before} or {size_after} rows \")\n",
    "sample.repartition(1).write.csv(os.path.join(data_dir,\"Model\", file), header=True)\n",
    "for outfile in os.listdir(os.path.join(data_dir, \"Model\", file)):\n",
    "    if outfile.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(data_dir, \"Model\",file ), os.path.join(data_dir, \"Model\", file ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23096972\n",
      "File test1.csv is sampled with size 0.029983800473932253 or 692535 rows \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mast30024asm1': conda)"
  },
  "interpreter": {
   "hash": "3ee96da6bb92bbf5b9d339a3dfb1419b22df50abca65b46f31ce3f2043457e99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}